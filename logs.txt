

########################## END RESULT ###############################
08/12/2021 13:06:15
While learning, 109702 out of 500000 episodes ended in success.
During evaluation, in 10 of 10 runs the agent managed to reach the goal while colliding 0 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
08/12/2021 19:50:15
While learning, 107948 out of 500000 episodes ended in success.
During evaluation, in 10 of 10 runs the agent managed to reach the goal while colliding 4 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 20, BPS Size: 50
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
09/12/2021 20:05:12
While learning, 104764 out of 500000 episodes ended in success.
During evaluation, in 10 of 10 runs the agent managed to reach the goal while colliding 3 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 20, BPS Size: 50
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
09/12/2021 20:26:06
While learning, 105324 out of 500000 episodes ended in success.
During evaluation, in 10 of 10 runs the agent managed to reach the goal while colliding 3 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 20, BPS Size: 50
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
09/12/2021 21:29:34
While learning, 58229 out of 500000 episodes ended in success.
During evaluation, in 1 of 10 runs the agent managed to reach the goal while colliding 101 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 40, BPS Size: 100
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
09/12/2021 21:54:52
While learning, 1683 out of 500000 episodes ended in success.
During evaluation, in 0 of 10 runs the agent managed to reach the goal while colliding 71 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 40, BPS Size: 250
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
10/12/2021 12:38:13
While learning, 109675 out of 500000 episodes ended in success.
During evaluation, in 10 of 10 runs the agent managed to reach the goal while colliding 1 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
10/12/2021 12:53:48
While learning, 113272 out of 500000 episodes ended in success.
During evaluation, in 10 of 10 runs the agent managed to reach the goal while colliding 3 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 5
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
10/12/2021 13:08:55
While learning, 113981 out of 500000 episodes ended in success.
During evaluation, in 10 of 10 runs the agent managed to reach the goal while colliding 4 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 3
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
10/12/2021 13:26:27
While learning, 108386 out of 500000 episodes ended in success.
During evaluation, in 80 of 80 runs the agent managed to reach the goal while colliding 14 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## IGNORE ###############################
10/01/2022 19:08:38
While learning, 108542 out of 500000 episodes ended in success.
During evaluation, in 10 of 10 runs the agent managed to reach the goal while colliding 0 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## PPO ###############################
10/01/2022 20:44:02
While learning, 15 out of 100 episodes ended in success.
During evaluation, in 0 of 10 runs the agent managed to reach the goal while colliding 1 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## PPO ###############################
10/01/2022 21:55:32
While learning, 112 out of 10000 episodes ended in success.
During evaluation, in 2 of 10 runs the agent managed to reach the goal while colliding 15 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## PPO ###############################
10/01/2022 22:04:37
While learning, 30 out of 10000 episodes ended in success.
During evaluation, in 1 of 10 runs the agent managed to reach the goal while colliding 6 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## PPO ###############################
10/01/2022 22:30:55
While learning, 55 out of 100000 episodes ended in success.
During evaluation, in 0 of 10 runs the agent managed to reach the goal while colliding 5 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## PPO ###############################
10/01/2022 22:56:30
While learning, 17 out of 100000 episodes ended in success.
During evaluation, in 2 of 10 runs the agent managed to reach the goal while colliding 1 times.
--------------------------Used variables:-------------------------
Grid size: 10, Object size: 0.2, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 10:12:46
While learning, 1897 out of 100000 episodes ended in success.
During evaluation, in 77 of 1000 runs the agent managed to reach the goal while colliding 301 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 10, BPS Size: 15
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 11:42:16
While learning, 2110 out of 500000 episodes ended in success.
During evaluation, in 44 of 1000 runs the agent managed to reach the goal while colliding 289 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 10, BPS Size: 20
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 12:18:08
While learning, 2417 out of 500000 episodes ended in success.
During evaluation, in 91 of 1000 runs the agent managed to reach the goal while colliding 294 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 10, BPS Size: 20
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 12:46:20
While learning, 2113 out of 500000 episodes ended in success.
During evaluation, in 70 of 1000 runs the agent managed to reach the goal while colliding 265 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 10, BPS Size: 30
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 13:17:45
While learning, 2142 out of 500000 episodes ended in success.
During evaluation, in 50 of 1000 runs the agent managed to reach the goal while colliding 297 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 10, BPS Size: 40
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 13:48:23
While learning, 2385 out of 500000 episodes ended in success.
During evaluation, in 45 of 1000 runs the agent managed to reach the goal while colliding 471 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 20
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 14:49:27
While learning, 2363 out of 500000 episodes ended in success.
During evaluation, in 59 of 1000 runs the agent managed to reach the goal while colliding 507 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 30
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 16:45:19
While learning, 2442 out of 500000 episodes ended in success.
During evaluation, in 84 of 1000 runs the agent managed to reach the goal while colliding 487 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 40
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 17:15:13
While learning, 2383 out of 500000 episodes ended in success.
During evaluation, in 92 of 1000 runs the agent managed to reach the goal while colliding 505 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 50
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 19:08:07
While learning, 2456 out of 500000 episodes ended in success.
During evaluation, in 51 of 1000 runs the agent managed to reach the goal while colliding 488 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 60
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 19:36:56
While learning, 2350 out of 500000 episodes ended in success.
During evaluation, in 45 of 1000 runs the agent managed to reach the goal while colliding 623 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 30
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 20:20:57
While learning, 2416 out of 500000 episodes ended in success.
During evaluation, in 34 of 1000 runs the agent managed to reach the goal while colliding 656 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 40
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 21:40:10
While learning, 2393 out of 500000 episodes ended in success.
During evaluation, in 47 of 1000 runs the agent managed to reach the goal while colliding 622 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 50
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
15/01/2022 22:33:22
While learning, 2565 out of 500000 episodes ended in success.
During evaluation, in 62 of 1000 runs the agent managed to reach the goal while colliding 603 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 60
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
16/01/2022 11:13:51
While learning, 2371 out of 500000 episodes ended in success.
During evaluation, in 65 of 1000 runs the agent managed to reach the goal while colliding 615 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 75
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
16/01/2022 11:52:53
While learning, 2573 out of 500000 episodes ended in success.
During evaluation, in 58 of 1000 runs the agent managed to reach the goal while colliding 606 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 90
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## END RESULT ###############################
16/01/2022 12:26:29
While learning, 1472 out of 500000 episodes ended in success.
During evaluation, in 18 of 1000 runs the agent managed to reach the goal while colliding 468 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 90
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
16/01/2022 21:55:20
While learning, 531040 out of 500000 episodes ended in success.
During evaluation, in 925 of 1000 runs the agent managed to reach the goal while colliding 47 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 50
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
16/01/2022 22:22:21
While learning, 541236 out of 500000 episodes ended in success.
During evaluation, in 950 of 1000 runs the agent managed to reach the goal while colliding 48 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 60
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
16/01/2022 22:51:47
While learning, 532494 out of 500000 episodes ended in success.
During evaluation, in 795 of 1000 runs the agent managed to reach the goal while colliding 82 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 70
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
16/01/2022 23:20:57
While learning, 528970 out of 500000 episodes ended in success.
During evaluation, in 608 of 1000 runs the agent managed to reach the goal while colliding 193 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 60
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
16/01/2022 23:44:20
While learning, 541499 out of 500000 episodes ended in success.
During evaluation, in 570 of 1000 runs the agent managed to reach the goal while colliding 141 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 75
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
17/01/2022 00:07:01
While learning, 534242 out of 500000 episodes ended in success.
During evaluation, in 581 of 1000 runs the agent managed to reach the goal while colliding 146 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 90
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
17/01/2022 09:34:25
While learning, 530991 out of 500000 episodes ended in success.
During evaluation, in 722 of 1000 runs the agent managed to reach the goal while colliding 109 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 120
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
17/01/2022 09:57:00
While learning, 528123 out of 500000 episodes ended in success.
During evaluation, in 429 of 1000 runs the agent managed to reach the goal while colliding 246 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 30, BPS Size: 140
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
17/01/2022 10:21:50
While learning, 535330 out of 500000 episodes ended in success.
During evaluation, in 615 of 1000 runs the agent managed to reach the goal while colliding 85 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 60
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
17/01/2022 10:50:34
While learning, 538686 out of 500000 episodes ended in success.
During evaluation, in 393 of 1000 runs the agent managed to reach the goal while colliding 104 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 60
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## NON-L REW ###############################
17/01/2022 11:20:12
While learning, 534476 out of 500000 episodes ended in success.
During evaluation, in 870 of 1000 runs the agent managed to reach the goal while colliding 56 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 60
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## OBJ SIZE ###############################
17/01/2022 14:08:30
While learning, 637732 out of 500000 episodes ended in success.
During evaluation, in 227 of 1000 runs the agent managed to reach the goal while colliding 159 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## OBJ SIZE ###############################
17/01/2022 14:17:56
While learning, 24624 out of 5000 episodes ended in success.
During evaluation, in 249 of 1000 runs the agent managed to reach the goal while colliding 290 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## OBJ SIZE ###############################
17/01/2022 15:02:09
While learning, 420696 out of 300000 episodes ended in success.
During evaluation, in 101 of 1000 runs the agent managed to reach the goal while colliding 118 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 10, BPS Size: 25
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################



########################## OBJ SIZE ###############################
17/01/2022 15:52:46
While learning, 684844 out of 600000 episodes ended in success.
During evaluation, in 110 of 1000 runs the agent managed to reach the goal while colliding 461 times.
--------------------------Used variables:-------------------------
Grid size: 1, Object size: 0.04, Num of obstacles: 20, BPS Size: 70
Negative reward coefficient: -0.2 (Multiplied with L-inf norm of BPS(agent_pos) to BPS(obstacles))
Positive reward: 100 (Multiplied with I(goal_reached))
#####################################################################

